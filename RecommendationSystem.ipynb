{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Transformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CosineSimilarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取数据构建正负样本对"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 针对每一个用户去获取他的评论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_user_data(data, top_n=2):\n",
    "    # 创建DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # 构建用户-酒店消费矩阵\n",
    "    user_hotel_matrix = df.pivot_table(index='user_id', columns='hotel_id', values='rating').fillna(0)\n",
    "\n",
    "    # 计算用户间的余弦相似度\n",
    "    user_similarity = cosine_similarity(user_hotel_matrix)\n",
    "\n",
    "    def find_similar_users(user_id, similarity_matrix, top_n=2):\n",
    "        user_index = user_hotel_matrix.index.get_loc(user_id)\n",
    "        sim_scores = list(enumerate(similarity_matrix[user_index]))\n",
    "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "        top_users = [i for i, _ in sim_scores[1:top_n+1]]\n",
    "        return [user_hotel_matrix.index[i] for i in top_users]\n",
    "\n",
    "    def build_samples(user_id, top_users, hotel_ratings, hotel_intros):\n",
    "        samples = []\n",
    "        for hotel, rating in hotel_ratings.items():\n",
    "            intro = hotel_intros.get(hotel, '')\n",
    "            if rating > 3:\n",
    "                samples.append((user_id, hotel, 1, intro))\n",
    "            else:\n",
    "                samples.append((user_id, hotel, 0, intro))\n",
    "\n",
    "            for similar_user in top_users:\n",
    "                similar_user_ratings = user_hotel_matrix.loc[similar_user].to_dict()\n",
    "                for hotel, rating in similar_user_ratings.items():\n",
    "                    if hotel not in hotel_ratings and rating > 3:\n",
    "                        intro = hotel_intros.get(hotel, '')\n",
    "                        samples.append((user_id, hotel, 1, intro))\n",
    "                    elif hotel not in hotel_ratings and rating <= 3:\n",
    "                        intro = hotel_intros.get(hotel, '')\n",
    "                        samples.append((user_id, hotel, 0, intro))\n",
    "\n",
    "        return samples\n",
    "\n",
    "    # 对所有用户构建样本\n",
    "    all_samples = []\n",
    "    for user_id in user_hotel_matrix.index:\n",
    "        top_users = find_similar_users(user_id, user_similarity, top_n)\n",
    "        hotel_ratings = user_hotel_matrix.loc[user_id].to_dict()\n",
    "        hotel_intros = df.set_index('hotel_id')['comment'].to_dict()\n",
    "        samples = build_samples(user_id, top_users, hotel_ratings, hotel_intros)\n",
    "        all_samples.extend(samples)\n",
    "\n",
    "    # 转换成DataFrame\n",
    "    samples_df = pd.DataFrame(all_samples, columns=['user_id', 'hotel_id', 'label', 'comment'])\n",
    "\n",
    "    # 转换类别特征\n",
    "    X = samples_df[['user_id', 'hotel_id', 'comment']]\n",
    "    X['user_id'] = X['user_id'].astype('category').cat.codes\n",
    "    X['hotel_id'] = X['hotel_id'].astype('category').cat.codes\n",
    "\n",
    "    y = samples_df.loc[:, ['label', 'user_id', 'hotel_id']]\n",
    "    \n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('/user_data')\n",
    "X_user,y_user=build_user_data(data)\n",
    "hotel_data=pd.read_csv('/hotel_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建输入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分组并将评论合并为列表\n",
    "df = pd.DataFrame(data)\n",
    "df2= pd.DataFrame(hotel_data)\n",
    "user_data = df.groupby('user_id')['comment'].apply(lambda comments: ' '.join(comments)).reset_index()\n",
    "hotel_intro_users=df.groupby('hotel_id')['comment'].apply(lambda comments: ' '.join(comments)).reset_index()\n",
    "hotel_intro_hotel=df2.groupby('hotel_id')['hotel_intro'].apply(lambda comments: ' '.join(comments)).reset_index()\n",
    "combined_df = pd.merge(hotel_intro_users, hotel_intro_hotel, on='hotel_id', how='outer')\n",
    "\n",
    "# 合并 'comment' 和 'hotel_intro' 列\n",
    "combined_df['text'] = combined_df.apply(lambda row: f\"{row['comment']}, {row['hotel_intro']}\" if pd.notna(row['comment']) and pd.notna(row['hotel_intro']) else (row['comment'] if pd.notna(row['comment']) else row['hotel_intro']), axis=1)\n",
    "\n",
    "# 选择最终需要的列\n",
    "hotel_data = combined_df[['hotel_id', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hotel_data.head(10))#hotel包含每个消费过的用户的评论以及酒店自己的描述\n",
    "print(user_data.head(10))#user包含自己消费过的酒店自己给出的评论"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 双塔模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super(RMSNorm, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = torch.sqrt(torch.mean(x**2, dim=-1, keepdim=True) + self.eps)\n",
    "        return self.weight * x / norm\n",
    "\n",
    "class RoPE(nn.Module):\n",
    "    def __init__(self, d_model, device, max_len=5000):\n",
    "        super(RoPE, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        position = torch.arange(seq_len, dtype=torch.float32, device=self.device).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2).float().to(self.device) * -(torch.log(torch.tensor(10000.0)).to(self.device) / self.d_model))\n",
    "        pos_enc = torch.zeros(seq_len, self.d_model, device=self.device)\n",
    "        pos_enc[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_enc[:, 1::2] = torch.cos(position * div_term)\n",
    "        return x + pos_enc[:seq_len]\n",
    "    \n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers, max_len=5000):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.rope = RoPE(d_model,device, max_len)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=2048, activation='gelu'),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.rmsnorm = RMSNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention_mask = (x.transpose(0,1)!= 0)\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x = self.rope(x)\n",
    "        x = self.transformer_encoder(x,src_key_padding_mask=~attention_mask)\n",
    "        x = self.rmsnorm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, user_data, hotel_data, y_user, tokenizer_name='bert-base-chinese', max_length=128):\n",
    "        self.user_data = user_data\n",
    "        self.hotel_data = hotel_data\n",
    "        self.y_user = y_user\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Create a mapping from user_id and hotel_id to indices\n",
    "        self.user_id_to_index = {uid: idx for idx, uid in enumerate(user_data['user_id'].unique())}\n",
    "        self.hotel_id_to_index = {hid: idx for idx, hid in enumerate(hotel_data['hotel_id'].unique())}\n",
    "        \n",
    "        # Create lists to store triplets\n",
    "        self.triplets = self._create_triplets()\n",
    "    \n",
    "    def _create_triplets(self):\n",
    "        triplets = []\n",
    "        user_to_hotels = self.y_user.groupby('user_id')['hotel_id'].apply(list).to_dict()\n",
    "        \n",
    "        for _, row in self.y_user.iterrows():\n",
    "            user_id = row['user_id']\n",
    "            hotel_id = row['hotel_id']\n",
    "            label = row['label']\n",
    "            \n",
    "            if label == 1:  # Positive sample\n",
    "                # Positive sample\n",
    "                anchor_index = self.user_id_to_index[user_id]\n",
    "                positive_index = self.hotel_id_to_index[hotel_id]\n",
    "                \n",
    "                # Negative sample\n",
    "                negative_hotels = [hid for hid in self.hotel_id_to_index.values() if hid != positive_index]\n",
    "                if len(negative_hotels) > 0:\n",
    "                    negative_index = np.random.choice(negative_hotels)\n",
    "                    triplets.append((anchor_index, positive_index, negative_index))\n",
    "        \n",
    "        return triplets\n",
    "    \n",
    "    def _tokenize(self, texts):\n",
    "        # Ensure all texts are padded/truncated to the same length\n",
    "        return self.tokenizer(texts, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        anchor_idx, positive_idx, negative_idx = self.triplets[index]\n",
    "        anchor_text = self.user_data['comment'].values[anchor_idx]\n",
    "        positive_text = self.hotel_data['hotel_intro'].values[positive_idx]\n",
    "        negative_text = self.hotel_data['hotel_intro'].values[negative_idx]\n",
    "        \n",
    "        # Tokenize texts\n",
    "        anchor_tokens = self._tokenize([anchor_text])\n",
    "        positive_tokens = self._tokenize([positive_text])\n",
    "        negative_tokens = self._tokenize([negative_text])\n",
    "        \n",
    "        return (anchor_tokens['input_ids'].squeeze(0), anchor_tokens['attention_mask'].squeeze(0)), \\\n",
    "               (positive_tokens['input_ids'].squeeze(0), positive_tokens['attention_mask'].squeeze(0)), \\\n",
    "               (negative_tokens['input_ids'].squeeze(0), negative_tokens['attention_mask'].squeeze(0))\n",
    "\n",
    "def create_dataloader(user_data, hotel_data, y_user, batch_size=3, shuffle=False, tokenizer_name='bert-base-chinese'):\n",
    "    dataset = TripletDataset(user_data, hotel_data, y_user, tokenizer_name=tokenizer_name)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return dataloader\n",
    "\n",
    "hotel_df=pd.DataFrame(hotel_data)\n",
    "\n",
    "dataloader = create_dataloader(user_data, hotel_df, y_user, batch_size=2, tokenizer_name='bert-base-chinese')\n",
    "\n",
    "# 打印一个batch的数据\n",
    "for batch in dataloader:\n",
    "    anchor_tokens, positive_tokens, negative_tokens = batch\n",
    "    print(\"Anchor tokens:\", anchor_tokens)\n",
    "    print(\"Positive tokens:\", positive_tokens)\n",
    "    print(\"Negative tokens:\", negative_tokens)\n",
    "    print(\"Anchor tokens type:\", type(anchor_tokens[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_model, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualTowerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers,hidden_dim,output_dim):\n",
    "        super(DualTowerModel, self).__init__()\n",
    "        self.user_encoder = TransformerEncoder(vocab_size, d_model, nhead, num_layers)\n",
    "        self.hotel_encoder= TransformerEncoder(vocab_size, d_model, nhead, num_layers)\n",
    "        self.mlp = MLP(d_model, hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input,pos,neg):\n",
    "        encoder_user = self.user_encoder(input)\n",
    "        representation_user = self.mlp(encoder_user)\n",
    "\n",
    "        encoder_pos = self.hotel_encoder(pos)\n",
    "        representation_pos = self.mlp(encoder_pos)\n",
    "\n",
    "        encoder_neg = self.hotel_encoder(neg)\n",
    "        representation_neg = self.mlp(encoder_neg)\n",
    "\n",
    "        return representation_user , representation_pos,representation_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained('bert-base-chinese')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vocab_size = tokenizer.vocab_size\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "num_layers = 6\n",
    "\n",
    "hidden_dim=256\n",
    "output_dim=128\n",
    "\n",
    "model=DualTowerModel(vocab_size, d_model, nhead, num_layers,hidden_dim,output_dim)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_triplet_loss(model, dataloader, optimizer, device, margin=1.0):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    #optimizer.to(device)\n",
    "    #margin.to(device)\n",
    "    criterion = nn.TripletMarginLoss(margin=margin, p=2).to(device)  # 使用 L2 距离\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        anchor_list, positive_list, negative_list = batch\n",
    "\n",
    "        anchor=torch.cat(anchor_list,dim=0)\n",
    "        positive=torch.cat(positive_list,dim=0)\n",
    "        negative=torch.cat(negative_list,dim=0)\n",
    "\n",
    "        anchor=anchor.to(device)\n",
    "        positive=positive.to(device)\n",
    "        negative=negative.to(device)\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        anchor_output,positive_output,negative_output = model(anchor,positive,negative)\n",
    "\n",
    "        print(anchor_output)\n",
    "        \n",
    "        # 计算 Triplet Loss\n",
    "        loss = criterion(anchor_output, positive_output, negative_output)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs=10\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    train_triplet_loss(model,dataloader,  optimizer, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "3c06e3e46abf38078fe4dac36a0085ec2b134ebbd73dd076183d243eeca6918f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
